{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a423b6-b84c-4f4f-a2ad-fdcc3eb7bc1d",
   "metadata": {},
   "source": [
    "This study employs the MIMIC-IV Note dataset, which comprises de-identified clinical data for research purposes. \n",
    "The dataset was received via PhysioNet, which is a restricted-access resource.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265571e2-0b9d-4d1f-8bb4-3bf85de752cf",
   "metadata": {},
   "source": [
    "# Pipeline-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6439d202-f92c-4245-8d66-f500dc98598f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saanidhya/anaconda3/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import modules\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "# Set the option to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "# Set the option to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee18006a-cbc5-4e73-9794-542c8b769069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this first\n",
    "\n",
    "# Extracts Diabetes related clinical notes\n",
    "\n",
    "\n",
    "#df = pd.read_csv('discharge.csv')\n",
    "# import re\n",
    "\n",
    "# # Compile a regex pattern for broader matching\n",
    "# pattern = re.compile(\n",
    "#     r'\\b(diabetes|diabetic|DM|T1DM|T2DM|type\\s+1\\s+diabetes|type\\s+2\\s+diabetes|'\n",
    "#     r'diabetes\\s+mellitus|gestational\\s+diabetes|insulin-dependent\\s+diabetes|'\n",
    "#     r'non-insulin-dependent\\s+diabetes|adult-onset\\s+diabetes|juvenile\\s+diabetes)\\b',\n",
    "#     flags=re.IGNORECASE\n",
    "# )\n",
    "\n",
    "# # Apply the filter\n",
    "# df_filtered = df[df['text'].apply(lambda x: bool(pattern.search(x)) if pd.notna(x) else False)]\n",
    "#df_filtered.to_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3c5257-c825-442b-8e07-347c61329da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b02e6b8-65e7-4282-bf1f-f747a5365d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124315, 9)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77d5d9a-c4c7-44f3-875f-4de35f687db1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>note_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>note_type</th>\n",
       "      <th>note_seq</th>\n",
       "      <th>charttime</th>\n",
       "      <th>storetime</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>10000980-DS-20</td>\n",
       "      <td>10000980</td>\n",
       "      <td>29654838</td>\n",
       "      <td>DS</td>\n",
       "      <td>20</td>\n",
       "      <td>2188-01-05 00:00:00</td>\n",
       "      <td>2188-01-06 20:49:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>10000980-DS-21</td>\n",
       "      <td>10000980</td>\n",
       "      <td>26913865</td>\n",
       "      <td>DS</td>\n",
       "      <td>21</td>\n",
       "      <td>2189-07-03 00:00:00</td>\n",
       "      <td>2189-07-03 19:50:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>10000980-DS-22</td>\n",
       "      <td>10000980</td>\n",
       "      <td>24947999</td>\n",
       "      <td>DS</td>\n",
       "      <td>22</td>\n",
       "      <td>2190-11-08 00:00:00</td>\n",
       "      <td>2190-11-09 13:57:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21</td>\n",
       "      <td>10000980-DS-23</td>\n",
       "      <td>10000980</td>\n",
       "      <td>25242409</td>\n",
       "      <td>DS</td>\n",
       "      <td>23</td>\n",
       "      <td>2191-04-11 00:00:00</td>\n",
       "      <td>2191-04-11 17:48:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22</td>\n",
       "      <td>10000980-DS-24</td>\n",
       "      <td>10000980</td>\n",
       "      <td>25911675</td>\n",
       "      <td>DS</td>\n",
       "      <td>24</td>\n",
       "      <td>2191-05-24 00:00:00</td>\n",
       "      <td>2191-05-24 17:29:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>10000980-DS-25</td>\n",
       "      <td>10000980</td>\n",
       "      <td>29659838</td>\n",
       "      <td>DS</td>\n",
       "      <td>25</td>\n",
       "      <td>2191-07-19 00:00:00</td>\n",
       "      <td>2191-07-22 09:37:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>10000980-DS-26</td>\n",
       "      <td>10000980</td>\n",
       "      <td>20897796</td>\n",
       "      <td>DS</td>\n",
       "      <td>26</td>\n",
       "      <td>2193-08-17 00:00:00</td>\n",
       "      <td>2193-08-17 16:05:00</td>\n",
       "      <td>\\nName:  ___          Unit No:   ___\\n \\nAdmi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>10001176-DS-17</td>\n",
       "      <td>10001176</td>\n",
       "      <td>23334588</td>\n",
       "      <td>DS</td>\n",
       "      <td>17</td>\n",
       "      <td>2186-12-02 00:00:00</td>\n",
       "      <td>2186-12-02 16:38:00</td>\n",
       "      <td>\\nName:  ___                   Unit No:   ___...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>41</td>\n",
       "      <td>10001667-DS-10</td>\n",
       "      <td>10001667</td>\n",
       "      <td>22672901</td>\n",
       "      <td>DS</td>\n",
       "      <td>10</td>\n",
       "      <td>2173-08-24 00:00:00</td>\n",
       "      <td>2173-08-24 15:04:00</td>\n",
       "      <td>\\nName:  ___                Unit No:   ___\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>44</td>\n",
       "      <td>10001877-DS-19</td>\n",
       "      <td>10001877</td>\n",
       "      <td>25679292</td>\n",
       "      <td>DS</td>\n",
       "      <td>19</td>\n",
       "      <td>2149-05-27 00:00:00</td>\n",
       "      <td>2149-05-27 11:29:00</td>\n",
       "      <td>\\nName:  ___                ___ No:   ___\\n \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0         note_id  subject_id   hadm_id note_type  note_seq  \\\n",
       "0          18  10000980-DS-20    10000980  29654838        DS        20   \n",
       "1          19  10000980-DS-21    10000980  26913865        DS        21   \n",
       "2          20  10000980-DS-22    10000980  24947999        DS        22   \n",
       "3          21  10000980-DS-23    10000980  25242409        DS        23   \n",
       "4          22  10000980-DS-24    10000980  25911675        DS        24   \n",
       "5          23  10000980-DS-25    10000980  29659838        DS        25   \n",
       "6          24  10000980-DS-26    10000980  20897796        DS        26   \n",
       "7          25  10001176-DS-17    10001176  23334588        DS        17   \n",
       "8          41  10001667-DS-10    10001667  22672901        DS        10   \n",
       "9          44  10001877-DS-19    10001877  25679292        DS        19   \n",
       "\n",
       "             charttime            storetime  \\\n",
       "0  2188-01-05 00:00:00  2188-01-06 20:49:00   \n",
       "1  2189-07-03 00:00:00  2189-07-03 19:50:00   \n",
       "2  2190-11-08 00:00:00  2190-11-09 13:57:00   \n",
       "3  2191-04-11 00:00:00  2191-04-11 17:48:00   \n",
       "4  2191-05-24 00:00:00  2191-05-24 17:29:00   \n",
       "5  2191-07-19 00:00:00  2191-07-22 09:37:00   \n",
       "6  2193-08-17 00:00:00  2193-08-17 16:05:00   \n",
       "7  2186-12-02 00:00:00  2186-12-02 16:38:00   \n",
       "8  2173-08-24 00:00:00  2173-08-24 15:04:00   \n",
       "9  2149-05-27 00:00:00  2149-05-27 11:29:00   \n",
       "\n",
       "                                                text  \n",
       "0   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "1   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "2   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "3   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "4   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "5   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "6   \\nName:  ___          Unit No:   ___\\n \\nAdmi...  \n",
       "7   \\nName:  ___                   Unit No:   ___...  \n",
       "8   \\nName:  ___                Unit No:   ___\\n ...  \n",
       "9   \\nName:  ___                ___ No:   ___\\n \\...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f5797dd-4f60-4191-82c8-ecb765bac633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '___' including any surrounding spaces with a single space and then trim excess spaces\n",
    "df_filtered['text'] = df_filtered['text'].str.replace(r'\\s*___\\s*', ' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efca362c-c88b-4ac1-9a29-c466e7cef8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to check for acute or active issues and mark them\n",
    "def mark_acute_issues(text):\n",
    "    # Regex pattern to identify sections\n",
    "    pattern = re.compile(r'(?<=\\n)((acute|active)(/acute|/active)?(\\s+or\\s+(acute|active))?(\\s+(issues|problems|conditions)))\\s*[:]?([\\s\\S]*?)(?=\\n[A-Z\\s]+:|\\Z)',re.IGNORECASE | re.MULTILINE)\n",
    "\n",
    "    # Search for matches\n",
    "    match = pattern.search(text)\n",
    "    return 1 if match else 0  # Return 1 if a match is found, else 0\n",
    "\n",
    "\n",
    "df_filtered['acute_issue_found'] = df_filtered['text'].apply(mark_acute_issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334aa43e-f73a-447f-8b5c-d9ca19148ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "acute_issue_found\n",
       "0    104006\n",
       "1     20309\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered['acute_issue_found'].value_counts()\n",
    "\n",
    "#output - acute_issue_found\n",
    "#0    104006\n",
    "#1     20309"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8dd82d8-3bfa-42f1-b494-06df9ffeee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove texts where there's no acute issues or its variations\n",
    "df_filtered = df_filtered[df_filtered['acute_issue_found'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912e83e-1990-4975-84f9-d33c6b53bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the early stages, when trying to catch all relevant texts concerning acute issues.\n",
    "# There were certain cases where it would either return too much or too little data\n",
    "# it was decided that capturing the entire data after acute_issues is found \n",
    "# then removing the unwanted headers and text would be more effective\n",
    "# This was done to preserve relevant information while removing unnecessary content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e868ae5-cd1e-4a44-b683-8dfc667d4b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the function to extract text from the mention of acute issues to the end of the document\n",
    "def extract_acute_issues(text):\n",
    "    \"\"\"\n",
    "    Extracts all text from the mention of acute issues until the end of the document, ignoring any section headers that follow.\n",
    " \n",
    "    \"\"\"\n",
    "    # Regex pattern to capture text from \"acute issues\" or variations until the end of the document\n",
    "    pattern = re.compile(r'(acute|active)\\s+(issues|conditions|problems)(?:[:\\s]*)([\\s\\S]*)', re.IGNORECASE)\n",
    "    \n",
    "    # Search for matches\n",
    "    match = pattern.search(text)\n",
    "    \n",
    "    # Return the match if found, otherwise return an empty string\n",
    "    return match.group(3) if match else \"\"\n",
    "\n",
    "\n",
    "df_filtered['acute_issues'] = df_filtered['text'].apply(extract_acute_issues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c87720d-04a3-4a24-ab4b-db97e3de4794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unwanted sections are now systematically removed \n",
    "# Function to clean text and flag\n",
    "def clean_headers(text):\n",
    "\n",
    "    # Define the regex pattern inside the function\n",
    "    exclusion_pattern = re.compile(\n",
    "        r'(?i)\\n('\n",
    "        r'(INACTIVE|CHRONIC|STABLE|TRANSITIONAL|DISCHARGE|'\n",
    "        r'MEDICATIONS|FOLLOWUP|RESOLVED)\\s*/?\\s*'\n",
    "        r'(ISSUES|PROBLEMS|CONDITIONS|DIAGNOSES|ISSUE|ILLNESSES|ILLNESS)|'\n",
    "        r'(CHRONIC|STABLE|RESOLVED)\\s*/?\\s*(CHRONIC|STABLE|RESOLVED)\\s*/?\\s*(CHRONIC|STABLE|RESOLVED)?\\s*/?\\s*'\n",
    "        r'(ISSUES|PROBLEMS|CONDITIONS|DIAGNOSES|ISSUE|ILLNESSES|ILLNESS)|'\n",
    "        r'CHRONIC\\s?&\\s?RESOLVED\\s*ISSUES|'  \n",
    "        r'Medications on Admission:|'\n",
    "        r'DISCHARGE LABS:|'\n",
    "        r'Past Medical History:|'\n",
    "        r'PROCEDURE DETAILS:|'\n",
    "        r'Discharge Medications:|'\n",
    "        r'CORE ISSUES:|'\n",
    "        r'CORE\\s*MEASURES|'\n",
    "        r'Treatment Plan|'\n",
    "        r'Medications for management:'\n",
    "        r')\\s*:?[\\s\\S]*$', re.DOTALL)\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = exclusion_pattern.search(text)\n",
    "    if match:\n",
    "        return text[:match.start()], 1\n",
    "    # Return the original text and flag 0 if no pattern is found\n",
    "    return text, 0\n",
    "\n",
    "\n",
    "df_filtered[['cleaned_text', 'flag']] = df_filtered['acute_issues'].apply(clean_headers).apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1b46d84-dd13-46bb-a3db-25760cd3881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still some unwanted headers had to be removed\n",
    "import re\n",
    "\n",
    "def remove_remaining_headers(text):\n",
    "    # Lowercase the text \n",
    "    text = text.lower()\n",
    "    \n",
    "    pattern = re.compile(r'^#(transitional issues|code status|contact|hcp/contact|code)', flags=re.MULTILINE)\n",
    "\n",
    "    # Search for the pattern\n",
    "    match = pattern.search(text)\n",
    "    if match:\n",
    "        # Find the index where the matched line starts\n",
    "        cut_off_index = match.start()\n",
    "        # Return the string from the start of the text to just before the matched line\n",
    "        return text[:cut_off_index]\n",
    "    return text\n",
    "\n",
    "df_filtered['cleaned_text'] = df_filtered['cleaned_text'].apply(remove_remaining_headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d40687-eed5-4f96-b9df-672f7bdfaee0",
   "metadata": {},
   "source": [
    "# preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a98db996-2160-4e9c-a0a6-d8b7dd1841f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_leading_period(text):\n",
    "    # Regex pattern to match a newline followed by any amount of whitespace and a period\n",
    "    # The pattern also includes any trailing whitespace after the period\n",
    "    pattern = re.compile(r'\\n\\s*\\.\\s*')\n",
    "    \n",
    "    # Replace the found pattern with just a newline \n",
    "    # This effectively removes the period and maintains the whitespace formatting\n",
    "    cleaned_text = re.sub(pattern, '\\n', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "df_filtered['cleaned_text'] = df_filtered['cleaned_text'].apply(remove_leading_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3607636c-667d-456c-949c-7868e738261c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#cleaning lines with only special characters\n",
    "#lines such as ######## \n",
    "import re\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    lines = text.split('\\n')\n",
    "    # Identify indices where lines consist solely of special characters\n",
    "    special_char_lines = [i for i, line in enumerate(lines) if re.match(r'^[\\W_]+$', line.strip())]\n",
    "\n",
    "    if len(special_char_lines) > 1:\n",
    "        # Get the index of the second occurrence of a special character line\n",
    "        second_occurrence = special_char_lines[1]\n",
    "        # Return everything up to but not including the line above the second occurrence\n",
    "        return '\\n'.join(lines[:max(0, second_occurrence - 1)])\n",
    "    elif len(special_char_lines) == 1:\n",
    "        # If there's only one occurrence, remove it\n",
    "        return '\\n'.join(line for i, line in enumerate(lines) if i != special_char_lines[0])\n",
    "    else:\n",
    "        # If there are no special character lines, return the original text\n",
    "        return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    final_text = remove_special_characters(text)\n",
    "    # Second pass: remove any remaining lines with only special characters if necessary\n",
    "    final_text = '\\n'.join(line for line in final_text.split('\\n') if not re.match(r'^[\\W_]+$', line.strip()))\n",
    "    return final_text\n",
    "\n",
    "df_filtered['transcript'] = df_filtered['cleaned_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f3a92b-7e7f-4663-a940-4df41e743300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove numerical index \n",
    "def remove_numerical_indices(text):\n",
    "    # Regex to match numbers at the start of a line followed by '.' or ')', ensuring it matches whole indices\n",
    "    # ^\\d+[\\.\\)] ensures matching numbers that are indices (like \"1.\" or \"2)\")\n",
    "    # Added \\s+ to ensure that it's followed by a space (which helps confirm it's an index)\n",
    "    text = re.sub(r'^\\d+[\\.\\)]\\s+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "df_filtered['transcript'] = df_filtered['transcript'].apply(remove_numerical_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a652fee7-290e-4146-8885-2570d6a92053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphens(text):\n",
    "    # Regex pattern to match a hyphen at the start of any line, preceded by whitespace\n",
    "    pattern = re.compile(r'(?m)^\\s*-')\n",
    "    \n",
    "   #remove the pattern\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    return cleaned_text\n",
    "df_filtered['transcript'] = df_filtered['transcript'].apply(remove_hyphens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d024ba3-77bc-43bf-818b-990a1bc4e706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract diagnosis based on the pattern\n",
    "# Usally most of the diagnosis are followed by clincal sentences\n",
    "# In this study, as the focus is only on clinical sentences; diagnosis aren't needed\n",
    "def extract_diagnosis(text):\n",
    "    pattern = re.compile(r'(^#.*?:|^#.*$|^[^#\\n]+?:)', re.MULTILINE)\n",
    "    matches = pattern.findall(text)\n",
    "    headings = []\n",
    "    seen = set()\n",
    "    for match in matches:\n",
    "        heading = match.strip()\n",
    "        if heading and heading not in seen:\n",
    "            seen.add(heading)\n",
    "            headings.append(heading)\n",
    "    return '\\n'.join(headings), pattern.sub('', text).strip()\n",
    "\n",
    "\n",
    "df_filtered[['diagnosis', 'transcript']] = df_filtered['transcript'].apply(lambda x: pd.Series(extract_diagnosis(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23eefe3d-a8fc-4011-b484-2843f96aabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess a single sentence\n",
    "def preprocess_sentence(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in punctuation and token not in stop_words]\n",
    "\n",
    "    # Join tokens back into a single string and return\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Function to split the text into sentences\n",
    "def preprocess_text(text):\n",
    "    sentences = sent_tokenize(text)  # Split into sentences\n",
    "    return sentences\n",
    "\n",
    "# Apply preprocessing to the transcript column to split into sentences and remove stopwords\n",
    "df_filtered['transcript'] = df_filtered['transcript'].apply(\n",
    "    lambda text: [preprocess_sentence(sentence) for sentence in preprocess_text(text)]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e71c169f-0fa1-43e4-a707-8925fa2b98fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 20309 entries, 12 to 124313\n",
      "Data columns (total 11 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  20309 non-null  int64 \n",
      " 1   note_id     20309 non-null  object\n",
      " 2   subject_id  20309 non-null  int64 \n",
      " 3   hadm_id     20309 non-null  int64 \n",
      " 4   note_type   20309 non-null  object\n",
      " 5   note_seq    20309 non-null  int64 \n",
      " 6   charttime   20309 non-null  object\n",
      " 7   storetime   20309 non-null  object\n",
      " 8   text        20309 non-null  object\n",
      " 9   transcript  20309 non-null  object\n",
      " 10  diagnosis   20309 non-null  object\n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 1.9+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#remove unnecessary columns\n",
    "# List of columns to drop\n",
    "columns_to_drop = ['acute_issue_found', 'acute_issues', 'cleaned_text', 'flag']\n",
    "\n",
    "# Drop the specified columns\n",
    "df_filtered.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(df_filtered.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a78e606-15e1-48e5-b693-d65f36873c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12    [patient presented without history urinary sys...\n",
      "20    [patient high coronary risk however recent cat...\n",
      "25    [pt endorsed 4mths pain rle became acutely wor...\n",
      "26    [patient presents severe right lower back pain...\n",
      "29    [placement presented inflammatory pericarditis...\n",
      "Name: transcript, dtype: object\n",
      "object\n"
     ]
    }
   ],
   "source": [
    "# Validating the results before further analysis\n",
    "print(df_filtered['transcript'].head())  # Inspect the first few entries\n",
    "print(df_filtered['transcript'].dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ce44150-d94f-4595-b2ee-928c3db61639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of 'transcript' column: <class 'list'>\n",
      "Content of the first entry in 'transcript' column: ['patient presented without history urinary systemic symptoms started ceftriaxone ed u/a positive 19 wbcs', 'antibiotics taken prior drawing urinary blood cultures yield', 'patient switched ciprofloxacin received three day total antibiotic course', 'ct scan performed evidence pyelonephritis', 'antibiotics discontinued time discharge', 'patient reported 3 weeks back/flank pain constant achy nature worsened movement', 'treated anti-inflammatories minimal effect', 'ct scan demonstrated nephrolithiasis', 'cxr showed bony abnormality could totally exclude multiple rib fractures', \"patient 's pain well controlled tolerating po medications discharged pcp following workup\", 'patient persistently last a1c 8.0. serum glucose initially 400s chem-7 gap however likely lactate unlikely dka given normal ph abg', 'glucose 218. continue home dose lantus 90 units qpm per records aggressive iss decrease uptitrate necessary depending ckd cr elevated 1.4 baseline 1.0. likely pre-renal setting infection', 's/p 2l ivf ed creatinine corrected 1.0. appears euvolemic maybe slightly', 'consider workup improvement urine lytes spinning urine renal u/s renally dose medications']\n"
     ]
    }
   ],
   "source": [
    "# Verify the type and content of the 'transcript' column\n",
    "#  each entry in 'transcript' should be a list\n",
    "print(\"Data type of 'transcript' column:\", type(df_filtered['transcript'].iloc[0]))\n",
    "print(\"Content of the first entry in 'transcript' column:\", df_filtered['transcript'].iloc[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a10e85d3-5192-4a73-ad46-fc4fe4810209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to hold individual sentences\n",
    "all_sentences = []\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df_filtered.iterrows():\n",
    "    # Iterate over each sentence in 'transcript' list\n",
    "    for sentence in row['transcript']:\n",
    "        # Append each sentence to the all_sentences list along with an index for reference\n",
    "        all_sentences.append({'original_index': index, 'sentence': sentence})\n",
    "\n",
    "# Create a new DataFrame from the list of sentences\n",
    "df_sentences = pd.DataFrame(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5855520f-b2fd-4e97-9c7c-49861c2f1744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(441492, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# returns 441,492 rows of clinical sentences\n",
    "df_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "398436de-9eaa-4341-b7f9-2376058fb8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the sentence column is of type string\n",
    "df_sentences['sentence'] = df_sentences['sentence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7efa4b23-8fb6-4e4a-ad1c-17f02eed8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbers could've influenced the semantic meaning of the sentences\n",
    "# In this case, number with compund units are removed so that measurements dont distort the results\n",
    "import re\n",
    "\n",
    "def preprocess_numbers(text):\n",
    "    # Pattern to remove numbers with units and compound units\n",
    "    pattern = r\"\\b\\d+\\.?\\d*\\s*(mg/kg|mg/dl|mg/L|ml/kg|mg|kg|ml|g|l|units|cm|mm|g/l|gnr|gpc)\\b|\\b\\d+\\.\\d+\\b|\\b\\d+\\b\"\n",
    "    \n",
    "    # Remove matching patterns\n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    \n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"[â€˜~\\\\\\+/\\.-]\", \"\", text)\n",
    "    \n",
    "    # Clean up extra spaces left after removals\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_sentences['sentence'] = df_sentences['sentence'].apply(preprocess_numbers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "964905d3-2b82-41d3-aa45-e3d75e0b6bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_possesives(text):\n",
    "    # Remove instances of \"'s\" without relying on word boundaries\n",
    "    text = re.sub(r\"'s\\b\", \"\", text)  # Matches 's at the end of a word\n",
    "    \n",
    "    # Remove instances of \"`\" followed by \"'\" around words\n",
    "    text = re.sub(r\"`\\s*'\\s*\", \"\", text)\n",
    "    \n",
    "    # Remove instances of \"''\" around any word\n",
    "    text = re.sub(r\"\\s*''\\s*\", \" \", text)\n",
    "    \n",
    "    # Clean up any extra spaces left after removals\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text\n",
    "df_sentences['sentence'] = df_sentences['sentence'].apply(cleaning_possesives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f312136-fd4d-46f9-b0ad-360bfa2222e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences with fewer than 3 words: 56120\n"
     ]
    }
   ],
   "source": [
    "###  removing short sentences\n",
    "\n",
    "#short sentences usually lack context or semantic meaning\n",
    "\n",
    "# Count the number of sentences with fewer than 3 words\n",
    "\n",
    "# These may negatively impact the clustering\n",
    "\n",
    "short_sentences_count = df_sentences['sentence'].apply(lambda x: len(x.split()) <= 3).sum()\n",
    "\n",
    "print(f\"Number of sentences with fewer than 3 words: {short_sentences_count}\")\n",
    "\n",
    "# output: Number of sentences with fewer than 3 words: 56120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59625d82-f5a9-40e8-87d7-ae89f311b560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences after filtering: 385372\n"
     ]
    }
   ],
   "source": [
    "# Filter out sentences with 3 or fewer words\n",
    "df_sentences_filtered = df_sentences[df_sentences['sentence'].apply(lambda x: len(x.split()) > 3)]\n",
    "\n",
    "# Check the number of rows after filtering\n",
    "print(f\"Number of sentences after filtering: {len(df_sentences_filtered)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca09adb8-a9c9-4560-938b-ff923d0a4742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "31c12253-b701-4af0-b382-8f30150afff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique sentences: 380292\n"
     ]
    }
   ],
   "source": [
    "# duplicate sentences are removed as they can skew the results later\\\n",
    "\n",
    "# Drop duplicate sentences\n",
    "df_sentences_filtered = df_sentences_filtered.drop_duplicates(subset='sentence', keep='first')\n",
    "\n",
    "# Reset the index after dropping duplicates \n",
    "df_sentences_filtered = df_sentences_filtered.reset_index(drop=True)\n",
    "\n",
    "# Display the number of remaining sentences\n",
    "print(f\"Number of unique sentences: {len(df_sentences_filtered)}\")\n",
    "\n",
    "#Number of unique sentences: 380292"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48cc9b-9e6e-4b8d-acf1-118c7247e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the file for embedding generation\n",
    "\n",
    "df_sentences_filtered.to_csv('sentences_vf.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
